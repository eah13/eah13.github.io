---
layout: post
link: https://vid2real.github.io/vid2realHRI
title: "Vid2RealHRI: Align video-based HRI with the real-world"
paper: true
---

*HRI research using autonomous robots in real-world settings can produce results with the highest ecological validity of any study modality, but many difficulties limit such studies' feasibility and effectiveness. We propose Vid2RealHRI, a research framework to maximize real-world insights offered by video-based studies.*

*The Vid2RealHRI framework was used to design an online study using first-person videos of robots as real-world encounter surrogates. The online study (n = 385) distinguished the within-subjects effects of four robot behavioral conditions on perceived social intelligence and human willingness to help the robot enter an exterior door. A real-world, between- subjects replication (n = 26) using two conditions confirmed the validity of the online study's findings and the sufficiency of the participant recruitment target (22) based on a power analysis of online study results. The Vid2RealHRI framework offers HRI researchers a principled way to take advantage of the efficiency of video-based study modalities while generating directly transferable knowledge of real-world HRI.*

*Collaborative work with Yao-Cheng Chan, Sadanand Modak, Joydeep Biswas, and Justin Hart.* 

*This work was supported by NSF grant #2219236 and UT Austin's Good Systems initiative.*
